{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Step 0 - Preprocessing</h1></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we first read the data including the bug reports and source code files of all 12 projects and for ease of access, we save them as two pickle files in the ./Output directory. Therefore, this set of code will populate the ./Output directory with \"allBugReports.pickle\" which is a pandas dataframe that contains all the bug reports from all projects and \"allSourceCodes.pickle\" which is a pandas dataframe that contains all source files after preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: javalang in /Users/justintijunelis/opt/anaconda3/lib/python3.8/site-packages (0.13.0)\n",
      "Requirement already satisfied: six in /Users/justintijunelis/opt/anaconda3/lib/python3.8/site-packages (from javalang) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install javalang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import warnings\n",
    "import javalang\n",
    "import re\n",
    "import glob\n",
    "import math\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "from tqdm.notebook import tqdm as tq\n",
    "from time import gmtime, strftime\n",
    "from random import randint\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Splitting code and natural language</h1></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Loading source codes into pandas Dataframe</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classNames_methodNames(node):\n",
    "    result=''\n",
    "    if isinstance(node,javalang.tree.MethodDeclaration) or isinstance(node,javalang.tree.ClassDeclaration):\n",
    "        return node.name.lower()+' '\n",
    "    if not (isinstance(node,javalang.tree.PackageDeclaration) or\n",
    "        isinstance(node,javalang.tree.FormalParameter) or\n",
    "       isinstance(node,javalang.tree.Import)):\n",
    "        if node:\n",
    "            if isinstance(node, javalang.ast.Node):\n",
    "                for childNode in node.children:\n",
    "                    result+=classNames_methodNames(childNode)\n",
    "    return result\n",
    "    \n",
    "def traverse_node(node,i=0):\n",
    "    i+=1\n",
    "    result=''\n",
    "    if not(isinstance(node,javalang.tree.PackageDeclaration)\n",
    "            or isinstance(node,javalang.tree.FormalParameter)            \n",
    "            or isinstance(node,javalang.tree.Import)\n",
    "            or isinstance(node,javalang.tree.CompilationUnit)):\n",
    "        if node:\n",
    "            if (isinstance(node,int) or isinstance(node,str) or isinstance(node,float)) and i==2:\n",
    "                result+=node+' '\n",
    "            if isinstance(node, javalang.ast.Node):\n",
    "                for childNode in node.children:\n",
    "                    result+=traverse_node(childNode,i)\n",
    "    return result\n",
    "\n",
    "def code_parser(code):\n",
    "    try:\n",
    "        tree = javalang.parse.parse(code)\n",
    "        return ''.join([traverse_node(node) for path, node in tree]) + ' ' + ''.join([classNames_methodNames(node)\n",
    "                                                                                      for path, node in tree])\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        return ''\n",
    "\n",
    "def loadSourceFiles2df(PATH,project):\n",
    "    \"\"\"\n",
    "    Receives: group name and project name \n",
    "    Process: open the source file directory and finds all the java files,\n",
    "             and after preprocessing(using code_preprocessor) load them into a pandas dataframe \n",
    "    Returns: dataframe >> \"filename\",\"code\",\"size\"\n",
    "    \"\"\"\n",
    "    print('Loading source files of {}  ...'.format(project))\n",
    "    PATH=os.path.join(\"data\",project,\"gitrepo\")\n",
    "    all_source_files=glob.glob(PATH+'/**/*.java', recursive=True)\n",
    "    source_codes_df=pd.DataFrame([])\n",
    "    sourceCodesList=[]\n",
    "\n",
    "    for filename in tq(all_source_files):\n",
    "        code=open(filename,encoding='ISO-8859-1').read()\n",
    "        if 'src/' in filename:\n",
    "            sourceCodesList.append(dict({\"filename\":filename.split('src/')[1].replace('/','.').lower(),\n",
    "                                         \"unprocessed_code\":code,\n",
    "                                         'project':project}))\n",
    "        else:\n",
    "            sourceCodesList.append(dict({\"filename\":filename.split(project)[1].replace('/','.').lower(),\n",
    "                                         \"unprocessed_code\":code,\n",
    "                                         'project':project}))\n",
    "    source_codes_df=source_codes_df.append(pd.DataFrame(sourceCodesList))\n",
    "    return source_codes_df\n",
    "\n",
    "def load_all_SCs(dataPath):\n",
    "    print('\\tLoading all source codes ... ')\n",
    "    source_codes_df=pd.DataFrame([])\n",
    "    all_projects= [folder for folder in listdir(dataPath)]\n",
    "    for project in all_projects:\n",
    "        source_path=os.path.join(dataPath,project,\"gitrepo\")\n",
    "        source_codes_df=source_codes_df.append(loadSourceFiles2df(source_path,project))\n",
    "    return source_codes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Loading bug reports pandas Dataframe</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadBugs2df(PATH,project):\n",
    "    \"\"\"\n",
    "    @Receives: the path to bug repository (the xml file)\n",
    "    @Process: Parses the xml file and reads the fix files per bug id. \n",
    "    @Returns: Returns the dataframe\n",
    "    \"\"\"\n",
    "    print(\"Loading Bug reports ... \")\n",
    "    all_bugs_df=pd.DataFrame([],columns=[\"id\",\"fix\",\"text\",\"fixdate\"])\n",
    "    bugRepo = ET.parse(PATH).getroot()\n",
    "    buglist=[]                   \n",
    "    for bug in tq(bugRepo.findall('bug')):\n",
    "        bugDict=dict({\"id\":bug.attrib['id'],\"fix\":[],\"fixdate\":bug.attrib['fixdate']\n",
    "                      ,\"summary\":None,\"description\":None,\"project\":project,\"average_precision\":0.0})\n",
    "        for bugDetail in bug.find('buginformation'):\n",
    "            if bugDetail.tag=='summary':\n",
    "                bugDict[\"summary\"]=bugDetail.text\n",
    "            elif bugDetail.tag=='description':\n",
    "                bugDict[\"description\"]=bugDetail.text\n",
    "        bugDict[\"fix\"]=np.array([fixFile.text.replace('/','.').lower() for fixFile in bug.find('fixedFiles')])\n",
    "        summary=str(bugDict['summary']) if str(bugDict['summary']) !=np.nan else \"\"\n",
    "        description=str(bugDict['description']) if str(bugDict['description']) !=np.nan else \"\"\n",
    "        buglist.append(bugDict)\n",
    "    all_bugs_df=all_bugs_df.append(pd.DataFrame(buglist))\n",
    "    return all_bugs_df.set_index('id')\n",
    "\n",
    "def load_all_BRs(dataPath):\n",
    "    print('\\tLoading all bug reports ... ')\n",
    "    all_bugs_df=pd.DataFrame([])\n",
    "    all_projects= [folder for folder in listdir(dataPath)]\n",
    "    for project in all_projects:\n",
    "        data_path=os.path.join(dataPath,project,\"bugrepo\",\"repository.xml\")\n",
    "        all_bugs_df=all_bugs_df.append(loadBugs2df(data_path,project))\n",
    "        print(len(all_bugs_df))\n",
    "    return all_bugs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><h1>Main Preprocessing class</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingUnit:\n",
    "    all_projects_source_codes=pd.DataFrame([])\n",
    "    all_projects_bugreports=pd.DataFrame([])\n",
    "    \n",
    "    def __init__(self,dataPath):\n",
    "        self.dataPath=dataPath\n",
    "        self.dataFolder=os.path.join(os.getcwd(),'Output')\n",
    "        if not os.path.exists(self.dataFolder):\n",
    "            os.makedirs(self.dataFolder)\n",
    "            \n",
    "    def execute(self):\n",
    "        self.loadEverything()\n",
    "\n",
    "    def loadEverything(self):\n",
    "        vectorize=False\n",
    "        if PreprocessingUnit.all_projects_bugreports.empty:\n",
    "            bugReportFile=os.path.join(self.dataFolder,'allBugReports.pickle')\n",
    "            if not os.path.isfile(bugReportFile):\n",
    "                PreprocessingUnit.all_projects_bugreports=load_all_BRs(dataPath=self.dataPath)\n",
    "                vectorize=True\n",
    "                PreprocessingUnit.all_projects_bugreports.to_pickle(bugReportFile)\n",
    "            else: \n",
    "                PreprocessingUnit.all_projects_bugreports=pd.read_pickle(bugReportFile)\n",
    "        print(\"*** All bug reports are are preprocessed and stored as: {} ***\".format('/'.join(bugReportFile.split('/')[-2:])))\n",
    "\n",
    "        if PreprocessingUnit.all_projects_source_codes.empty:\n",
    "            sourceCodeFile=os.path.join(self.dataFolder,'allSourceCodes.pickle')\n",
    "            if not os.path.isfile(sourceCodeFile):\n",
    "                PreprocessingUnit.all_projects_source_codes=load_all_SCs(dataPath=self.dataPath)\n",
    "                vectorize=True\n",
    "                PreprocessingUnit.all_projects_source_codes.to_pickle(sourceCodeFile)\n",
    "            else:\n",
    "                PreprocessingUnit.all_projects_source_codes=pd.read_pickle(sourceCodeFile)\n",
    "        print(\"*** All source codes are preprocessed and stored as: {} ***\".format('/'.join(sourceCodeFile.split('/')[-2:])))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** All bug reports are are preprocessed and stored as: Output/allBugReports.pickle ***\n",
      "*** All source codes are preprocessed and stored as: Output/allSourceCodes.pickle ***\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    config={'DATA_PATH':os.path.join('data')}\n",
    "    preprocessor=PreprocessingUnit(dataPath=config['DATA_PATH'])\n",
    "    preprocessor.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** All Bug Reports are Loaded. ***\n",
      "*** All Source Codes are Loaded. ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['org.apache.commons.lang.builder.equalsbuildertest.java',\n",
       "       'org.apache.commons.lang.builder.equalsbuilder.java'], dtype='<U54')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "filename            main.java.org.springframework.security.authent...\n",
       "unprocessed_code    /* Copyright 2004, 2005, 2006 Acegi Technology...\n",
       "project                                                           SEC\n",
       "Name: 428, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def loadEverything():\n",
    "    all_projects_bugreports = pd.read_pickle('Output/allBugReports.pickle')\n",
    "    print(\"*** All Bug Reports are Loaded. ***\")\n",
    "    all_projects_source_codes = pd.read_pickle('Output/allSourceCodes.pickle')\n",
    "    print(\"*** All Source Codes are Loaded. ***\")\n",
    "    return all_projects_bugreports, all_projects_source_codes\n",
    "\n",
    "all_projects_bugreports, all_projects_source_codes = loadEverything()\n",
    "display(all_projects_bugreports.iloc[1000]['fix'])\n",
    "display(all_projects_source_codes.iloc[1000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem\n",
    "\n",
    "There are several software engineering (SE) problems that can be investigated using machine learning. Among them, we will be working on a problem called \"Fault Localization\" (FL). The goal of FL is to automatically locate a fault entity (e.g. a source file, a class, a method, etc) in source code. There are different variations of FL and we will focus on Information Retrieval based FL (IRFL). This article explains FL: https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=2530&context=sis_research. \n",
    "\n",
    "In short, the idea is, given a new bug report document, we want to automatically identify the source code file that most likely needs a fix, so we can save time for debugging. \n",
    "\n",
    "To do this, we may use the previous bug resports and identify the locations (files) that have been patched as our training set. So, we build an IRFL model that:\n",
    "\n",
    "- Finds the textual similarity between the new bug report and the historical ones. \n",
    "- Then rank historically patched source files based on how similar their bug reports are to the new bug report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's import some things that will help us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/justintijunelis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/justintijunelis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Key Imports\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from collections import defaultdict\n",
    "\n",
    "import multiprocessing as mp\n",
    "from itertools import repeat\n",
    "\n",
    "# Download some stuff to run the code\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "cores = mp.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "\n",
    "First, we need to create our training set. Using `all_projects_bug_reports` and `all_projects_source_codes`.\n",
    "\n",
    "We will clean the bug report and source code text by creating a function that:\n",
    "\n",
    "1. Makes all text lowercase\n",
    "2. Removes all punctuation from the text\n",
    "3. Removes all repetitive white space from the text\n",
    "4. Tokenizes the filtered string and removes stem words\n",
    "\n",
    "Then, we will extract the features and labels of the bug report by:\n",
    "\n",
    "1. Concatenating the bug summary and description, then using the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  # Remove imports\n",
    "  text = re.sub(r\"import\\s.*;\", \"\", text)\n",
    "  # Remove packages\n",
    "  text = re.sub(r\"package\\s.*;\", \"\", text)\n",
    "  # Space out camel case (https://stackoverflow.com/questions/5020906/python-convert-camel-case-to-space-delimited-using-regex-and-taking-acronyms-in)\n",
    "  text = re.sub(r\"((?<=[a-z])[A-Z]|(?<!\\A)[A-Z](?=[a-z]))\", r\" \\1\", text)\n",
    "  # Replace punctuation with spaces (https://stackoverflow.com/questions/68590438/replace-punctuation-with-space-in-text)\n",
    "  text = re.sub(r\"(?:[^\\w\\s]|_)+\", \" \", text)\n",
    "  # Replace white space or repeating whitespace with single space\n",
    "  text = re.sub(\"\\s+\", \" \", text) \n",
    "  # Remove all numbers\n",
    "  text = re.sub(r\"[0-9]+\", \"\", text)\n",
    "  # Remove HTML tags? # Remove static, int, char, etc\n",
    "  \n",
    "  # Make everything lowercase\n",
    "  text = text.lower()\n",
    "\n",
    "  # Tokenize the words and remove all stop words, ensure no punctuation goes though\n",
    "  banned_tokens = [\"copyright\", \"void\"]\n",
    "  tokenized = word_tokenize(text) \n",
    "  tokens = []\n",
    "  for token in tokenized:\n",
    "    if token not in stopwords.words('english') and token not in banned_tokens:\n",
    "      tokens.append(token)\n",
    "\n",
    "  # Stem all words using Porter Stemming\n",
    "  stemmer = PorterStemmer()\n",
    "  tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "  # Recreate the text\n",
    "  return \" \".join(tokens)\n",
    "\n",
    "def process_bug_reports(bug_reports):\n",
    "  for row, bug_report in bug_reports.iterrows():\n",
    "    description = bug_report['description']\n",
    "    summary = bug_report['summary']\n",
    "    bug_text = \"\"\n",
    "    if isinstance(description, str):\n",
    "      bug_text += description\n",
    "    if isinstance(summary, str):\n",
    "      bug_text += summary\n",
    "    bug_text = clean_text(bug_text)\n",
    "\n",
    "    # Skip and delete the row if the bug text is empty\n",
    "    if bug_text == \"\":\n",
    "      bug_reports.drop(row, inplace = True)\n",
    "    else:\n",
    "      bug_reports.at[row, 'text'] = bug_text\n",
    "  bug_reports.drop('summary', axis = 1, inplace = True)\n",
    "  bug_reports.drop('description', axis = 1, inplace = True)\n",
    "  return bug_reports\n",
    "\n",
    "def process_source_files(source_files):\n",
    "  source_files['unprocessed_code'] = source_files['unprocessed_code'].apply(lambda t: clean_text(t))\n",
    "  source_files.rename(columns={\"unprocessed_code\": \"code\"}, inplace = True)\n",
    "  source_files = source_files[source_files['code'] != \"\"]\n",
    "  return source_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the bug reports and source files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  with mp.get_context('fork').Pool(processes = cores) as p:\n",
    "    bug_reports = pd.concat(\n",
    "      p.map(\n",
    "        process_bug_reports, \n",
    "        np.array_split(all_projects_bugreports.copy(), cores)\n",
    "      )\n",
    "    )\n",
    "  with mp.get_context('fork').Pool(processes = cores) as p:\n",
    "    source_files = pd.concat(\n",
    "      p.map(\n",
    "        process_source_files, \n",
    "        np.array_split(all_projects_source_codes.copy(), cores)\n",
    "      )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1\n",
    "- You preprocess the data to have a clean dataset representing source files (including the buggy ones) and the bug reports. The exact preprocessing choices are ours to make.\n",
    "- Next, apply the TF-IDF method to calculate the similarity between the new bug report (to locate) and the source code files directly. Unlike BugLocator, we ignore the historical bug reports in this step. The similarity function of Method 1 is called the direct relevancy function.\n",
    "- Finally, we rank the source files based on their textual similarity to the new bug report and present the results using proper evaluation metrics (such as MAP and MRR).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Similarities\n",
    "\n",
    "Since our dataset has multiple projects, and each project has multiple bug reports, we don't want to compare bug reports with files of a project they don't belong to. So, we will compare a bug report to all the files in its respective project. \n",
    "\n",
    "To compute similarity between a bug report and it's project file:\n",
    "\n",
    "1. Iterate through each file of the bug report's project.\n",
    "2. Create a TF-IDF vectorizer and fit and transform the file's source code since we want to compare against the source code.\n",
    "3. Transform the bug report's text with the vectorizer.\n",
    "4. Compare the similarity of the two resulting vectors using cosine distance.\n",
    "\n",
    "We will iterate through each bug report and generate its similarity, then return a list of similarities that will implicitely map to each bug report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(bug_report, source_files):\n",
    "  # Find the similarity score\n",
    "  vectorizer = TfidfVectorizer()\n",
    "  bug_vector = vectorizer.fit_transform([bug_report['text']])\n",
    "  source_vector = vectorizer.transform(source_files['code'])\n",
    "  similarity_score = cosine_similarity(source_vector, bug_vector)\n",
    "\n",
    "  # Extract the score\n",
    "  scores = []\n",
    "  for score in similarity_score:\n",
    "    scores.append(score[0])\n",
    "\n",
    "  # Sort the values\n",
    "  df = pd.DataFrame()\n",
    "  df['scores'] = np.array(scores)\n",
    "  df['files'] = source_files['filename'].values\n",
    "  df = df.sort_values('scores')\n",
    "\n",
    "  # Create a tuple array\n",
    "  return (df['files'], df['scores'])\n",
    "\n",
    "def compute_similarities(bug_reports, source_files):\n",
    "  for row, bug_report in bug_reports.iterrows():\n",
    "    similarity = calculate_similarity(bug_report, source_files)\n",
    "    bug_reports.at[row, 'similarities'] = [similarity]\n",
    "  return bug_reports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the similarities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fix</th>\n",
       "      <th>text</th>\n",
       "      <th>fixdate</th>\n",
       "      <th>project</th>\n",
       "      <th>average_precision</th>\n",
       "      <th>similarities</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>[org.apache.commons.collections.map.flat3map.j...</td>\n",
       "      <td>flat map amp apo entri object overwrit entri a...</td>\n",
       "      <td>2006-07-18 22:02:11</td>\n",
       "      <td>COLLECTIONS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>([test.org.apache.commons.collections.bag.abst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>[org.apache.commons.collections.testextendedpr...</td>\n",
       "      <td>field includ extend properti current static pr...</td>\n",
       "      <td>2006-07-18 22:44:33</td>\n",
       "      <td>COLLECTIONS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[([java.org.apache.commons.collections.priorit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>[org.apache.commons.collections.testlistutils....</td>\n",
       "      <td>remov collect collect collect remov method cal...</td>\n",
       "      <td>2006-08-18 19:01:22</td>\n",
       "      <td>COLLECTIONS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[([test.org.apache.commons.collections.testfas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>[org.apache.commons.collections.map.flat3map.j...</td>\n",
       "      <td>final flat map new flat map put new integ new ...</td>\n",
       "      <td>2007-08-20 14:11:54</td>\n",
       "      <td>COLLECTIONS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[([java.org.apache.commons.collections.resetta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>[org.apache.commons.collections.fasttreemap.java]</td>\n",
       "      <td>line current releas replac map new tree map ma...</td>\n",
       "      <td>2007-08-31 09:39:59</td>\n",
       "      <td>COLLECTIONS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[([test.org.apache.commons.collections.bag.abs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12661</th>\n",
       "      <td>[org.springframework.core.annotation.annotated...</td>\n",
       "      <td>statu quo use test ng deriv test abstract test...</td>\n",
       "      <td>2015-01-24 09:20:46</td>\n",
       "      <td>SPR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[([main.java.org.springframework.validation.pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11598</th>\n",
       "      <td>[org.springframework.core.annotation.annotated...</td>\n",
       "      <td>background issu pick spr left statu quo implem...</td>\n",
       "      <td>2015-05-13 07:20:33</td>\n",
       "      <td>SPR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[([main.java.org.springframework.jms.support.c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14080</th>\n",
       "      <td>[org.springframework.web.util.uricomponentsbui...</td>\n",
       "      <td>play part spr natur default cor processor chec...</td>\n",
       "      <td>2016-03-24 13:17:04</td>\n",
       "      <td>SPR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[([main.java.org.springframework.aop.intercept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14248</th>\n",
       "      <td>[org.springframework.web.util.uricomponentsbui...</td>\n",
       "      <td>chang http jira spring io brows spr introduc d...</td>\n",
       "      <td>2016-05-05 15:01:10</td>\n",
       "      <td>SPR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[([main.java.org.springframework.transaction.j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14678</th>\n",
       "      <td>[org.springframework.web.servlet.mvc.condition...</td>\n",
       "      <td>use path variabl like request map valu test te...</td>\n",
       "      <td>2016-09-08 13:08:20</td>\n",
       "      <td>SPR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[([main.java.org.springframework.beans.factory...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1858 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     fix  \\\n",
       "id                                                         \n",
       "217    [org.apache.commons.collections.map.flat3map.j...   \n",
       "214    [org.apache.commons.collections.testextendedpr...   \n",
       "222    [org.apache.commons.collections.testlistutils....   \n",
       "261    [org.apache.commons.collections.map.flat3map.j...   \n",
       "264    [org.apache.commons.collections.fasttreemap.java]   \n",
       "...                                                  ...   \n",
       "12661  [org.springframework.core.annotation.annotated...   \n",
       "11598  [org.springframework.core.annotation.annotated...   \n",
       "14080  [org.springframework.web.util.uricomponentsbui...   \n",
       "14248  [org.springframework.web.util.uricomponentsbui...   \n",
       "14678  [org.springframework.web.servlet.mvc.condition...   \n",
       "\n",
       "                                                    text              fixdate  \\\n",
       "id                                                                              \n",
       "217    flat map amp apo entri object overwrit entri a...  2006-07-18 22:02:11   \n",
       "214    field includ extend properti current static pr...  2006-07-18 22:44:33   \n",
       "222    remov collect collect collect remov method cal...  2006-08-18 19:01:22   \n",
       "261    final flat map new flat map put new integ new ...  2007-08-20 14:11:54   \n",
       "264    line current releas replac map new tree map ma...  2007-08-31 09:39:59   \n",
       "...                                                  ...                  ...   \n",
       "12661  statu quo use test ng deriv test abstract test...  2015-01-24 09:20:46   \n",
       "11598  background issu pick spr left statu quo implem...  2015-05-13 07:20:33   \n",
       "14080  play part spr natur default cor processor chec...  2016-03-24 13:17:04   \n",
       "14248  chang http jira spring io brows spr introduc d...  2016-05-05 15:01:10   \n",
       "14678  use path variabl like request map valu test te...  2016-09-08 13:08:20   \n",
       "\n",
       "           project  average_precision  \\\n",
       "id                                      \n",
       "217    COLLECTIONS                0.0   \n",
       "214    COLLECTIONS                0.0   \n",
       "222    COLLECTIONS                0.0   \n",
       "261    COLLECTIONS                0.0   \n",
       "264    COLLECTIONS                0.0   \n",
       "...            ...                ...   \n",
       "12661          SPR                0.0   \n",
       "11598          SPR                0.0   \n",
       "14080          SPR                0.0   \n",
       "14248          SPR                0.0   \n",
       "14678          SPR                0.0   \n",
       "\n",
       "                                            similarities  \n",
       "id                                                        \n",
       "217    ([test.org.apache.commons.collections.bag.abst...  \n",
       "214    [([java.org.apache.commons.collections.priorit...  \n",
       "222    [([test.org.apache.commons.collections.testfas...  \n",
       "261    [([java.org.apache.commons.collections.resetta...  \n",
       "264    [([test.org.apache.commons.collections.bag.abs...  \n",
       "...                                                  ...  \n",
       "12661  [([main.java.org.springframework.validation.pa...  \n",
       "11598  [([main.java.org.springframework.jms.support.c...  \n",
       "14080  [([main.java.org.springframework.aop.intercept...  \n",
       "14248  [([main.java.org.springframework.transaction.j...  \n",
       "14678  [([main.java.org.springframework.beans.factory...  \n",
       "\n",
       "[1858 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "  project_bug_reports = [group for _, group in bug_reports.groupby('project')]\n",
    "  project_source_files = [group for _, group in source_files.groupby('project')]\n",
    "  with mp.get_context('fork').Pool(processes = len(project_bug_reports)) as p:\n",
    "    bug_reports = pd.concat(\n",
    "      p.starmap(\n",
    "        compute_similarities,\n",
    "        zip(project_bug_reports, project_source_files)\n",
    "      )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's check the MAP and MRR scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MRR for the COLLECTIONS (92 files) is:0.4385393301594366\n",
      "The MRR for the CONFIGURATION (133 files) is:0.0971564160826495\n",
      "The MRR for the DATACMNS (158 files) is:0.12316258936201992\n",
      "The MRR for the DATAMONGO (271 files) is:0.22803984217608048\n",
      "The MRR for the DATAREST (132 files) is:0.15343346732163798\n",
      "The MRR for the ELY (25 files) is:0.8052925396106277\n",
      "The MRR for the IO (91 files) is:0.28804934264230403\n",
      "The MRR for the LANG (217 files) is:0.2698903731687933\n",
      "The MRR for the LDAP (53 files) is:0.09240995260378969\n",
      "The MRR for the SEC (541 files) is:0.3854120000444161\n",
      "The MRR for the SOCIALFB (15 files) is:0.0933496468068902\n",
      "The MRR for the SPR (130 files) is:0.0715529101040166\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calc_MRR(bug_report):\n",
    "  adjusted_reciprocals = []\n",
    "  fixed_files = bug_report['fix']\n",
    "  project_file_similarities = bug_report['similarities']\n",
    "  # Compute an MRR for each file in the fix list\n",
    "  for offset, fixed_file in enumerate(fixed_files):\n",
    "    # Find the ranking of the fixed file in the similarities array\n",
    "    similarity_index = 0\n",
    "    for i, similarity in enumerate(project_file_similarities):\n",
    "      file, _ = similarity\n",
    "      if file in fixed_file:\n",
    "        similarity_index = i\n",
    "        break\n",
    "    # Calculate the similarity\n",
    "    reciprocal = 1 / max(1, similarity_index - len(fixed_files) - offset - 1)\n",
    "    adjusted_reciprocals.append(reciprocal)\n",
    "\n",
    "  return np.mean(reciprocal)\n",
    "\n",
    "def calculate_adjusted_project_MRRs(bug_reports):\n",
    "  pass\n",
    "\n",
    "def calculate_adjusted_MRR(bug_report):\n",
    "  reciprocals = []\n",
    "  # Compute the reciprocal for each bug report\n",
    "  for _, bug_report in bug_reports.iterrows():\n",
    "    report_reciprocals = []\n",
    "    fixed_files = bug_report['fix']\n",
    "    bug_similarities = bug_report['similarities']\n",
    "    # Compute the adjusted recriprocal for each fixed file\n",
    "    for j, file in enumerate(fixed_files):\n",
    "      # Find the index of the file in similarities\n",
    "      file_index = 0\n",
    "      for k, f in enumerate(bug_similarities['files']):\n",
    "        if file in f:\n",
    "          file_index = k\n",
    "          break\n",
    "\n",
    "      # Compute the the adjusted reciprocal\n",
    "      reciprocal = 1 / max(1, file_index - len(fixed_files) - j + 1)\n",
    "      report_reciprocals.append(reciprocal)\n",
    "\n",
    "    reciprocals.append(np.mean(report_reciprocals))\n",
    "  return np.mean(reciprocals)\n",
    "\n",
    "def calculate_project_MRRs(bug_reports):\n",
    "  # Group the bug reports by prject\n",
    "  project_bug_reports = [group for _, group in bug_reports.groupby('project')]\n",
    "\n",
    "  # Calcuate the MRR for each project\n",
    "  for bug_report in project_bug_reports:\n",
    "    # Print our results\n",
    "    print(\n",
    "      \"The MRR for the \" + \n",
    "      bug_report['project'][0] + \n",
    "      \" (\" + \n",
    "      str(len(bug_report['similarities'])) + \n",
    "      \" files\" + \n",
    "      \") is: \",\n",
    "      end = \"\"\n",
    "    )\n",
    "    print(calculate_adjusted_MRR(reports, similarity['similarities']))\n",
    "\n",
    "calculate_project_MRRs(bug_reports)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2\n",
    "\n",
    "In this step, we will develop a new IRFL method and comparing to Method 1.\n",
    "\n",
    "We will roughly implement the BugLocator tool. We will use the same preprocessing as TF-IDF code we developed for method 1 to calculate an indirect relevancy function. Then, we will use a weighted average of the direct relevancy function and indirect relevancy function to do the ranking for this method. The indirect function calculates the similarity between the new bug report and the historical ones. Then, given that we already know which exact files have been fixed for each historical bug report. So, we can map files to historical bug reports. Then, the algorithm ranks source files according to their indirect similarity (the similarity of a source file's corresponding historical report(s) to the new bug report) to the new bug report.\n",
    "\n",
    "- Method 2 MUST improve method 1 results.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 3\n",
    "\n",
    "This is our brand new FL technique applicable on this dataset. The novel approach should use a machine learning/information retrieval method that is not taught in class. It is okay if the method is already proposed in the FL literature and is published, however, your code cannot be copy-pasted. This method does not need to outperform the other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59492509ee09bb1e98cb3b73aca52e57ed875f73ef434eaac26b9b50866a2d0e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
