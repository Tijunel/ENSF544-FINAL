{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Step 0 - Preprocessing</h1></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we first read the data including the bug reports and source code files of all 12 projects and for ease of access, we save them as two pickle files in the ./Output directory. Therefore, this set of code will populate the ./Output directory with \"allBugReports.pickle\" which is a pandas dataframe that contains all the bug reports from all projects and \"allSourceCodes.pickle\" which is a pandas dataframe that contains all source files after preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: javalang in /Users/justintijunelis/opt/anaconda3/lib/python3.8/site-packages (0.13.0)\n",
      "Requirement already satisfied: six in /Users/justintijunelis/opt/anaconda3/lib/python3.8/site-packages (from javalang) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install javalang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import warnings\n",
    "import javalang\n",
    "import re\n",
    "import glob\n",
    "import math\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "import multiprocessing\n",
    "from tqdm.notebook import tqdm as tq\n",
    "from time import gmtime, strftime\n",
    "from random import randint\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Splitting code and natural language</h1></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Loading source codes into pandas Dataframe</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classNames_methodNames(node):\n",
    "    result=''\n",
    "    if isinstance(node,javalang.tree.MethodDeclaration) or isinstance(node,javalang.tree.ClassDeclaration):\n",
    "        return node.name.lower()+' '\n",
    "    if not (isinstance(node,javalang.tree.PackageDeclaration) or\n",
    "        isinstance(node,javalang.tree.FormalParameter) or\n",
    "       isinstance(node,javalang.tree.Import)):\n",
    "        if node:\n",
    "            if isinstance(node, javalang.ast.Node):\n",
    "                for childNode in node.children:\n",
    "                    result+=classNames_methodNames(childNode)\n",
    "    return result\n",
    "    \n",
    "def traverse_node(node,i=0):\n",
    "    i+=1\n",
    "    result=''\n",
    "    if not(isinstance(node,javalang.tree.PackageDeclaration)\n",
    "            or isinstance(node,javalang.tree.FormalParameter)            \n",
    "            or isinstance(node,javalang.tree.Import)\n",
    "            or isinstance(node,javalang.tree.CompilationUnit)):\n",
    "        if node:\n",
    "            if (isinstance(node,int) or isinstance(node,str) or isinstance(node,float)) and i==2:\n",
    "                result+=node+' '\n",
    "            if isinstance(node, javalang.ast.Node):\n",
    "                for childNode in node.children:\n",
    "                    result+=traverse_node(childNode,i)\n",
    "    return result\n",
    "\n",
    "def code_parser(code):\n",
    "    try:\n",
    "        tree = javalang.parse.parse(code)\n",
    "        return ''.join([traverse_node(node) for path, node in tree]) + ' ' + ''.join([classNames_methodNames(node)\n",
    "                                                                                      for path, node in tree])\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        return ''\n",
    "\n",
    "def loadSourceFiles2df(PATH,project):\n",
    "    \"\"\"\n",
    "    Receives: group name and project name \n",
    "    Process: open the source file directory and finds all the java files,\n",
    "             and after preprocessing(using code_preprocessor) load them into a pandas dataframe \n",
    "    Returns: dataframe >> \"filename\",\"code\",\"size\"\n",
    "    \"\"\"\n",
    "    print('Loading source files of {}  ...'.format(project))\n",
    "    PATH=os.path.join(\"data\",project,\"gitrepo\")\n",
    "    all_source_files=glob.glob(PATH+'/**/*.java', recursive=True)\n",
    "    source_codes_df=pd.DataFrame([])\n",
    "    sourceCodesList=[]\n",
    "\n",
    "    for filename in tq(all_source_files):\n",
    "        code=open(filename,encoding='ISO-8859-1').read()\n",
    "        if 'src/' in filename:\n",
    "            sourceCodesList.append(dict({\"filename\":filename.split('src/')[1].replace('/','.').lower(),\n",
    "                                         \"unprocessed_code\":code,\n",
    "                                         'project':project}))\n",
    "        else:\n",
    "            sourceCodesList.append(dict({\"filename\":filename.split(project)[1].replace('/','.').lower(),\n",
    "                                         \"unprocessed_code\":code,\n",
    "                                         'project':project}))\n",
    "    source_codes_df=source_codes_df.append(pd.DataFrame(sourceCodesList))\n",
    "    return source_codes_df\n",
    "\n",
    "def load_all_SCs(dataPath):\n",
    "    print('\\tLoading all source codes ... ')\n",
    "    source_codes_df=pd.DataFrame([])\n",
    "    all_projects= [folder for folder in listdir(dataPath)]\n",
    "    for project in all_projects:\n",
    "        source_path=os.path.join(dataPath,project,\"gitrepo\")\n",
    "        source_codes_df=source_codes_df.append(loadSourceFiles2df(source_path,project))\n",
    "    return source_codes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Loading bug reports pandas Dataframe</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadBugs2df(PATH,project):\n",
    "    \"\"\"\n",
    "    @Receives: the path to bug repository (the xml file)\n",
    "    @Process: Parses the xml file and reads the fix files per bug id. \n",
    "    @Returns: Returns the dataframe\n",
    "    \"\"\"\n",
    "    print(\"Loading Bug reports ... \")\n",
    "    all_bugs_df=pd.DataFrame([],columns=[\"id\",\"fix\",\"text\",\"fixdate\"])\n",
    "    bugRepo = ET.parse(PATH).getroot()\n",
    "    buglist=[]                   \n",
    "    for bug in tq(bugRepo.findall('bug')):\n",
    "        bugDict=dict({\"id\":bug.attrib['id'],\"fix\":[],\"fixdate\":bug.attrib['fixdate']\n",
    "                      ,\"summary\":None,\"description\":None,\"project\":project,\"average_precision\":0.0})\n",
    "        for bugDetail in bug.find('buginformation'):\n",
    "            if bugDetail.tag=='summary':\n",
    "                bugDict[\"summary\"]=bugDetail.text\n",
    "            elif bugDetail.tag=='description':\n",
    "                bugDict[\"description\"]=bugDetail.text\n",
    "        bugDict[\"fix\"]=np.array([fixFile.text.replace('/','.').lower() for fixFile in bug.find('fixedFiles')])\n",
    "        summary=str(bugDict['summary']) if str(bugDict['summary']) !=np.nan else \"\"\n",
    "        description=str(bugDict['description']) if str(bugDict['description']) !=np.nan else \"\"\n",
    "        buglist.append(bugDict)\n",
    "    all_bugs_df=all_bugs_df.append(pd.DataFrame(buglist))\n",
    "    return all_bugs_df.set_index('id')\n",
    "\n",
    "def load_all_BRs(dataPath):\n",
    "    print('\\tLoading all bug reports ... ')\n",
    "    all_bugs_df=pd.DataFrame([])\n",
    "    all_projects= [folder for folder in listdir(dataPath)]\n",
    "    for project in all_projects:\n",
    "        data_path=os.path.join(dataPath,project,\"bugrepo\",\"repository.xml\")\n",
    "        all_bugs_df=all_bugs_df.append(loadBugs2df(data_path,project))\n",
    "        print(len(all_bugs_df))\n",
    "    return all_bugs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><h1>Main Preprocessing class</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingUnit:\n",
    "    all_projects_source_codes=pd.DataFrame([])\n",
    "    all_projects_bugreports=pd.DataFrame([])\n",
    "    \n",
    "    def __init__(self,dataPath):\n",
    "        self.dataPath=dataPath\n",
    "        self.dataFolder=os.path.join(os.getcwd(),'Output')\n",
    "        if not os.path.exists(self.dataFolder):\n",
    "            os.makedirs(self.dataFolder)\n",
    "            \n",
    "    def execute(self):\n",
    "        self.loadEverything()\n",
    "\n",
    "    def loadEverything(self):\n",
    "        vectorize=False\n",
    "        if PreprocessingUnit.all_projects_bugreports.empty:\n",
    "            bugReportFile=os.path.join(self.dataFolder,'allBugReports.pickle')\n",
    "            if not os.path.isfile(bugReportFile):\n",
    "                PreprocessingUnit.all_projects_bugreports=load_all_BRs(dataPath=self.dataPath)\n",
    "                vectorize=True\n",
    "                PreprocessingUnit.all_projects_bugreports.to_pickle(bugReportFile)\n",
    "            else: \n",
    "                PreprocessingUnit.all_projects_bugreports=pd.read_pickle(bugReportFile)\n",
    "        print(\"*** All bug reports are are preprocessed and stored as: {} ***\".format('/'.join(bugReportFile.split('/')[-2:])))\n",
    "\n",
    "        if PreprocessingUnit.all_projects_source_codes.empty:\n",
    "            sourceCodeFile=os.path.join(self.dataFolder,'allSourceCodes.pickle')\n",
    "            if not os.path.isfile(sourceCodeFile):\n",
    "                PreprocessingUnit.all_projects_source_codes=load_all_SCs(dataPath=self.dataPath)\n",
    "                vectorize=True\n",
    "                PreprocessingUnit.all_projects_source_codes.to_pickle(sourceCodeFile)\n",
    "            else:\n",
    "                PreprocessingUnit.all_projects_source_codes=pd.read_pickle(sourceCodeFile)\n",
    "        print(\"*** All source codes are preprocessed and stored as: {} ***\".format('/'.join(sourceCodeFile.split('/')[-2:])))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** All bug reports are are preprocessed and stored as: Output/allBugReports.pickle ***\n",
      "*** All source codes are preprocessed and stored as: Output/allSourceCodes.pickle ***\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    config={'DATA_PATH':os.path.join('data')}\n",
    "    preprocessor=PreprocessingUnit(dataPath=config['DATA_PATH'])\n",
    "    preprocessor.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** All Bug Reports are Loaded. ***\n",
      "*** All Source Codes are Loaded. ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fix                  [org.apache.commons.lang.builder.equalsbuilder...\n",
       "text                                                               NaN\n",
       "fixdate                                            2008-01-13 07:00:40\n",
       "summary              EqualsBuilder don&apos;t compare BigDecimals c...\n",
       "description          When comparing a BigDecimal, the comparing is ...\n",
       "project                                                           LANG\n",
       "average_precision                                                  0.0\n",
       "Name: 393, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "filename            main.java.org.springframework.security.authent...\n",
       "unprocessed_code    /* Copyright 2004, 2005, 2006 Acegi Technology...\n",
       "project                                                           SEC\n",
       "Name: 428, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def loadEverything():\n",
    "    all_projects_bugreports = pd.read_pickle('Output/allBugReports.pickle')\n",
    "    print(\"*** All Bug Reports are Loaded. ***\")\n",
    "    all_projects_source_codes = pd.read_pickle('Output/allSourceCodes.pickle')\n",
    "    print(\"*** All Source Codes are Loaded. ***\")\n",
    "    return all_projects_bugreports, all_projects_source_codes\n",
    "\n",
    "all_projects_bugreports, all_projects_source_codes = loadEverything()\n",
    "display(all_projects_bugreports.iloc[1000])\n",
    "display(all_projects_source_codes.iloc[1000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem\n",
    "\n",
    "There are several software engineering (SE) problems that can be investigated using machine learning. Among them, we will be working on a problem called \"Fault Localization\" (FL). The goal of FL is to automatically locate a fault entity (e.g. a source file, a class, a method, etc) in source code. There are different variations of FL and we will focus on Information Retrieval based FL (IRFL). This article explains FL: https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=2530&context=sis_research. \n",
    "\n",
    "In short, the idea is, given a new bug report document, we want to automatically identify the source code file that most likely needs a fix, so we can save time for debugging. \n",
    "\n",
    "To do this, we may use the previous bug resports and identify the locations (files) that have been patched as our training set. So, we build an IRFL model that:\n",
    "\n",
    "- Finds the textual similarity between the new bug report and the historical ones. \n",
    "- Then rank historically patched source files based on how similar their bug reports are to the new bug report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's import some things that will help us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/justintijunelis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/justintijunelis/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Key Imports\n",
    "import re, string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from collections import defaultdict\n",
    "\n",
    "# Download some stuff to run the code\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "\n",
    "First, we need to create our training set. Using `all_projects_bug_reports` and `all_projects_source_codes`.\n",
    "\n",
    "We will clean the bug report and source code text by creating a function that:\n",
    "\n",
    "1. Makes all text lowercase\n",
    "2. Removes all punctuation from the text\n",
    "3. Removes all repetitive white space from the text\n",
    "4. Tokenizes the filtered string and removes stem words\n",
    "\n",
    "Then, we will extract the features and labels of the bug report by:\n",
    "\n",
    "1. Concatenating the bug summary and description, then using the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  \"\"\"\n",
    "  Processes the given text by changing all text to lowercase, removing punctuation, removing repetitive \n",
    "  whitespace, tokenizing words, and finally stemming all words.\n",
    "  Return a string containing tokenized stem words separated by a single space.\n",
    "  \"\"\"\n",
    "  # Change text to lower case\n",
    "  # text = text.lower()\n",
    "\n",
    "  # Remove imports\n",
    "  text = re.sub(r\"import\\s.*;\", \"\", text)\n",
    "  \n",
    "  # Remove packages\n",
    "  text = re.sub(r\"package\\s.*;\", \"\", text)\n",
    "\n",
    "  # Space out camel case (https://stackoverflow.com/questions/5020906/python-convert-camel-case-to-space-delimited-using-regex-and-taking-acronyms-in)\n",
    "  text = re.sub(r\"((?<=[a-z])[A-Z]|(?<!\\A)[A-Z](?=[a-z]))\", r\" \\1\", text)\n",
    "\n",
    "  # Replace punctuation with spaces (https://stackoverflow.com/questions/68590438/replace-punctuation-with-space-in-text)\n",
    "  text = re.sub(r\"(?:[^\\w\\s]|_)+\", \" \", text)\n",
    "\n",
    "  # Replace white space or repeating whitespace with single space\n",
    "  text = re.sub(\"\\s+\", \" \", text) \n",
    "\n",
    "  # Remove all numbers\n",
    "  text = re.sub(r\"[0-9]+\", \"\", text)\n",
    "  \n",
    "  # Make everything lowercase\n",
    "  text = text.lower()\n",
    "\n",
    "  # Tokenize the words and remove all stop words, ensure no punctuation goes though\n",
    "  tokenized = word_tokenize(text)\n",
    "  tokens = []\n",
    "  for token in tokenized:\n",
    "    if token not in stopwords.words('english'):\n",
    "      tokens.append(token)\n",
    "\n",
    "  # Stem all words using Porter Stemming\n",
    "  stemmer = PorterStemmer()\n",
    "  tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "  # Recreate the text\n",
    "  return \" \".join(tokens)\n",
    "\n",
    "def process_bug_reports(bug_reports):\n",
    "  clean_bug_reports = {\n",
    "    \"fix\": [],\n",
    "    \"project\": [],\n",
    "    \"text\": [],\n",
    "    \"fixdate\": []\n",
    "  }\n",
    "  for _, bug_report in bug_reports.iterrows():\n",
    "    # Concatenate the report's description and summary\n",
    "    description = bug_report['description']\n",
    "    summary = bug_report['summary']\n",
    "    bug_text = \"\"\n",
    "    if isinstance(description, str):\n",
    "      bug_text += description\n",
    "    if isinstance(summary, str):\n",
    "      bug_text += summary\n",
    "\n",
    "    # If the bug report is empty, we should not consider it.\n",
    "    if bug_text == \"\":\n",
    "      continue\n",
    "    \n",
    "    bug_text = clean_text(bug_text)\n",
    "\n",
    "    # Append to the clean_bug_report dictionary\n",
    "    clean_bug_reports['fix'].append(bug_report['fix'])\n",
    "    clean_bug_reports['project'].append(bug_report['project'])\n",
    "    clean_bug_reports['text'].append(bug_text)\n",
    "    clean_bug_reports['fixdate'].append(bug_report['fixdate'])\n",
    "\n",
    "  # Put the dict into a dataframe and return\n",
    "  clean_bug_reports['index'] = range(len(clean_bug_reports['fix']))\n",
    "  clean_bug_reports = pd.DataFrame.from_dict(clean_bug_reports).set_index('index')\n",
    "  return clean_bug_reports\n",
    "\n",
    "def process_source_files(source_files):\n",
    "  clean_source_files = {\n",
    "    \"filename\": [],\n",
    "    \"code\": [],\n",
    "    \"project\": []\n",
    "  }\n",
    "  for _, source_file in source_files.iterrows():\n",
    "    # Clean the source file's code\n",
    "    clean_code = clean_text(source_file['unprocessed_code'])\n",
    "\n",
    "    # Append to the clean_source_files dictionary\n",
    "    clean_source_files['filename'].append(source_file['filename'])\n",
    "    clean_source_files['code'].append(clean_code)\n",
    "    clean_source_files['project'].append(source_file['project'])\n",
    "\n",
    "  # Put the dict into a dataframe and return\n",
    "  clean_source_files['index'] = range(len(clean_source_files['filename']))\n",
    "  clean_source_files = pd.DataFrame.from_dict(clean_source_files).set_index('index')\n",
    "  return clean_source_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's process the bug reports and source files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " copyright  the original author or authors licensed under the apache license version   the license you may not use this file except in compliance with the license you may obtain a copy of the license at http www apache org licenses license   unless required by applicable law or agreed to in writing software distributed under the license is distributed on an as is basis without warranties or conditions of any kind either express or implied see the license for the specific language governing permissions and limitations under the license integration tests for link hal browser author oliver gierke soundtrack miles davis blue in green kind of blue run with spring j unit class runner class web app configuration context configuration public class hal browser integration tests static final string base path api static final string browser index browser index html static final string target base path concat browser index concat concat base path configuration enable web mvc static class test configuration extends repository rest mvc configuration override protected void configure repository rest configuration repository rest configuration config config set base path base path autowired web application context context mock mvc mvc before public void set up this mvc mock mvc builders web app context setup context default request get base path accept media type text html build see datarest  test public void exposes json under api root by default throws exception mvc perform get base path accept media type all and expect status is ok and expect header string http headers content type starts with media types hal json to string see datarest  test public void redirects to browser for api root and html throws exception mvc perform get base path accept media type text html and expect status is found and expect header string http headers location ends with target see datarest  test public void forwards browser to index html throws exception mvc perform get base path concat browser and expect status is found and expect header string http headers location ends with target see datarest  test public void exposes hal browser throws exception mvc perform get base path concat browser index html and expect status is ok and expect content string contains string the hal browser see datarest  test public void retruns api if html is not explicitly listed throws exception mvc perform get base path accept media type application json media type all and expect status is ok and expect header string http headers content type starts with media type application json value \n",
      " copyright   the original author or authors licensed under the apache license version   the license you may not use this file except in compliance with the license you may obtain a copy of the license at http www apache org licenses license   unless required by applicable law or agreed to in writing software distributed under the license is distributed on an as is basis without warranties or conditions of any kind either express or implied see the license for the specific language governing permissions and limitations under the license unit tests for link hal browser author oliver gierke soundtrack nils wã¼lker homeless diamond feat lauren flynn public class hal browser unit tests see datarest  see datarest  test public void creates context relative redirect for browser throws exception mock http servlet response response new mock http servlet response mock http servlet request request new mock http servlet request request set request uri context request set context path context view view new hal browser browser request assert that view is instance of redirect view class abstract view view render collections string object empty map request response uri components components uri components builder from uri string response get header http headers location build assert that components get path starts with context assert that components get fragment is context test public void produces proxy relative redirect if necessary mock http servlet request request new mock http servlet request get browser request add header x forwarded host somehost request add header x forwarded port  request add header x forwarded proto https request add header x forwarded prefix prefix view view new hal browser browser request assert that view is instance of redirect view class string url redirect view view get url assert that url starts with https somehost  prefix assert that url ends with prefix \n",
      " copyright   the original author or authors licensed under the apache license version   the license you may not use this file except in compliance with the license you may obtain a copy of the license at http www apache org licenses license   unless required by applicable law or agreed to in writing software distributed under the license is distributed on an as is basis without warranties or conditions of any kind either express or implied see the license for the specific language governing permissions and limitations under the license controller with a few convenience redirects to expose the hal browser shipped as static content author oliver gierke soundtrack miles davis so what kind of blue base path aware controller public class hal browser private static string browser browser private static string index index html redirects requests to the api root asking for html to the hal browser return request mapping value method request method get produces media type text html value public view index http servlet request request return get redirect view request false redirects to the actual code index html return request mapping value browser method request method get public view browser http servlet request request return get redirect view request request get request uri ends with browser returns the view to redirect to to access the hal browser param request must not be literal null param browser relative return private view get redirect view http servlet request request boolean browser relative servlet uri components builder builder servlet uri components builder from request request uri components components builder build string path components get path null components get path if browser relative builder path browser builder path index builder fragment browser relative path substring  path last index of browser path return new redirect view builder build to uri string \n",
      " author jon brisbin class http entity matcher t extends base matcher http entity t private final http entity t expected public http entity matcher http entity t expected assert not null expected http entity cannot be null this expected expected public static t http entity matcher t http entity http entity t http entity return new http entity matcher t http entity override public boolean matches object item if item instanceof http entity return false if item instanceof response entity expected instanceof response entity response entity left response entity expected response entity right response entity item if left get status code equals right get status code return false http entity left expected http entity right http entity item return left get body equals right get body left get headers equals right get headers override public void describe to description description description append text expected to string \n",
      " copyright   the original author or authors licensed under the apache license version   the license you may not use this file except in compliance with the license you may obtain a copy of the license at http www apache org licenses license   unless required by applicable law or agreed to in writing software distributed under the license is distributed on an as is basis without warranties or conditions of any kind either express or implied see the license for the specific language governing permissions and limitations under the license integration tests for link root resource information author oliver gierke run with spring j unit class runner class context configuration classes jpa repository config class transactional public class root resource information integration tests extends abstract controller integration tests see datarest  test public void get is not supported if find all is not exported supported http methods supported methods get resource information address class get supported methods assert that supported methods get methods for collection not has item get see datarest  test public void post is not supported if save is not exported supported http methods supported methods get resource information address class get supported methods assert that supported methods get methods for collection not has item post \n",
      "copyright origin author author licens apach licens version licens may use file except complianc licens may obtain copi licens http www apach org licens licens unless requir applic law agre write softwar distribut licens distribut basi without warranti condit kind either express impli see licens specif languag govern permiss limit licens integr test link hal browser author oliv gierk soundtrack mile davi blue green kind blue run spring j unit class runner class web app configur context configur public class hal browser integr test static final string base path api static final string browser index browser index html static final string target base path concat browser index concat concat base path configur enabl web mvc static class test configur extend repositori rest mvc configur overrid protect void configur repositori rest configur repositori rest configur config config set base path base path autowir web applic context context mock mvc mvc public void set mvc mock mvc builder web app context setup context default request get base path accept media type text html build see datarest test public void expos json api root default throw except mvc perform get base path accept media type expect statu ok expect header string http header content type start media type hal json string see datarest test public void redirect browser api root html throw except mvc perform get base path accept media type text html expect statu found expect header string http header locat end target see datarest test public void forward browser index html throw except mvc perform get base path concat browser expect statu found expect header string http header locat end target see datarest test public void expos hal browser throw except mvc perform get base path concat browser index html expect statu ok expect content string contain string hal browser see datarest test public void retrun api html explicitli list throw except mvc perform get base path accept media type applic json media type expect statu ok expect header string http header content type start media type applic json valu\n"
     ]
    }
   ],
   "source": [
    "#bugs = process_bug_reports(all_projects_bugreports)\n",
    "source_files = process_source_files(all_projects_source_codes.head())\n",
    "print(source_files['code'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1\n",
    "- You preprocess the data to have a clean dataset representing source files (including the buggy ones) and the bug reports. The exact preprocessing choices are ours to make.\n",
    "- Next, apply the TF-IDF method to calculate the similarity between the new bug report (to locate) and the source code files directly. Unlike BugLocator, we ignore the historical bug reports in this step. The similarity function of Method 1 is called the direct relevancy function.\n",
    "- Finally, we rank the source files based on their textual similarity to the new bug report and present the results using proper evaluation metrics (such as MAP and MRR).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Similarities\n",
    "\n",
    "Since our dataset has multiple projects, and each project has multiple bug reports, we don't want to compare bug reports with files of a project they don't belong to. So, we will compare a bug report to all the files in its respective project. \n",
    "\n",
    "To compute similarity between a bug report and it's project file:\n",
    "\n",
    "1. Iterate through each file of the bug report's project.\n",
    "2. Create a TF-IDF vectorizer and fit and transform the file's source code since we want to compare against the source code.\n",
    "3. Transform the bug report's text with the vectorizer.\n",
    "4. Compare the similarity of the two resulting vectors using cosine distance.\n",
    "\n",
    "We will iterate through each bug report and generate its similarity, then return a list of similarities that will implicitely map to each bug report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'copyright origin author author licens apach licens version licens may use file except complianc licens may obtain copi licens httpwwwapacheorglicenseslicens unless requir applic law agre write softwar distribut licens distribut basi without warranti condit kind either express impli see licens specif languag govern permiss limit licens packag orgspringframeworkdatarestwebmvchalbrows import static orghamcrestcorematch import static orgspringframeworktestwebservletrequestmockmvcrequestbuild import static orgspringframeworktestwebservletresultmockmvcresultmatch import orgjunitbefor import orgjunittest import orgjunitrunnerrunwith import orgspringframeworkbeansfactoryannotationautowir import orgspringframeworkcontextannotationconfigur import orgspringframeworkdatarestcoreconfigrepositoryrestconfigur import orgspringframeworkdatarestwebmvcconfigrepositoryrestmvcconfigur import orgspringframeworkhateoasmediatyp import orgspringframeworkhttphttphead import orgspringframeworkhttpmediatyp import orgspringframeworktestcontextcontextconfigur import orgspringframeworktestcontextjunit springjunit classrunn import orgspringframeworktestcontextwebwebappconfigur import orgspringframeworktestwebservletmockmvc import orgspringframeworktestwebservletsetupmockmvcbuild import orgspringframeworkwebcontextwebapplicationcontext import orgspringframeworkwebservletconfigannotationenablewebmvc integr test link halbrows author oliv gierk soundtrack mile davi blue green kind blue runwithspringjunit classrunnerclass webappconfigur contextconfigur public class halbrowserintegrationtest static final string basepath api static final string browserindex browserindexhtml static final string target basepathconcatbrowserindexconcatconcatbasepath configur enablewebmvc static class testconfigur extend repositoryrestmvcconfigur overrid protect void configurerepositoryrestconfigurationrepositoryrestconfigur config configsetbasepathbasepath autowir webapplicationcontext context mockmvc mvc public void setup thismvc mockmvcbuilderswebappcontextsetupcontext defaultrequestgetbasepathacceptmediatypetexthtmlbuild see datarest test public void exposesjsonunderapirootbydefault throw except mvcperformgetbasepathacceptmediatypeal andexpectstatusisok andexpectheaderstringhttpheaderscontenttyp startswithmediatypeshaljsontostr see datarest test public void redirectstobrowserforapirootandhtml throw except mvcperformgetbasepathacceptmediatypetexthtml andexpectstatusisfound andexpectheaderstringhttpheadersloc endswithtarget see datarest test public void forwardsbrowsertoindexhtml throw except mvcperformgetbasepathconcatbrows andexpectstatusisfound andexpectheaderstringhttpheadersloc endswithtarget see datarest test public void exposeshalbrows throw except mvcperformgetbasepathconcatbrowserindexhtml andexpectstatusisok andexpectcontentstringcontainsstringth hal browser see datarest test public void retrunsapiifhtmlisnotexplicitlylist throw except mvcperformgetbasepathacceptmediatypeapplicationjson mediatypeal andexpectstatusisok andexpectheaderstringhttpheaderscontenttyp startswithmediatypeapplicationjsonvalu'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calculate_similarity(bug_report, source_files):\n",
    "  \"\"\"\n",
    "  Calculates the text of a bug report to the set of source files \n",
    "  WITHIN THE SAME PROJECT AS THE bug_report.\n",
    "  \"\"\"\n",
    "  similarity = {\n",
    "    \"scores\": [],\n",
    "    \"files\": []\n",
    "  }\n",
    "  # For each file, we will calculate the similarity of it's source code to the bug report\n",
    "  for file in source_files:\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    source_vector = vectorizer.fit_transform([file['code']])\n",
    "    bug_vector = vectorizer.transform([bug_report['text']])\n",
    "    similarity_score = cosine_similarity(source_vector, bug_vector)[0][0]\n",
    "    similarity['scores'].append(similarity_score)\n",
    "    similarity['files'].append(file['filename'])\n",
    "\n",
    "  return pd.DataFrame.from_dict(similarity)\n",
    "\n",
    "def compute_similarities(bug_reports, source_files):\n",
    "  # First, let's find the source files for each project\n",
    "  project_files = defaultdict(list)\n",
    "  for _, source_file in source_files.iterrows():\n",
    "    project_files[source_file['project']].append(source_file)\n",
    "\n",
    "  # Then, let's compute the similarities for each bug with its projects source files\n",
    "  similarities = []\n",
    "  for _, bug_report in bug_reports.iterrows():\n",
    "    print(bug_report['project'])\n",
    "    similarity = calculate_similarity(bug_report, project_files[bug_report['project']])\n",
    "    similarities.append(similarity)\n",
    "\n",
    "  return similarities\n",
    "\n",
    "# Calculate the similarities\n",
    "#similarities = compute_similarities(bugs, source_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2\n",
    "\n",
    "In this step, we will develop a new IRFL method and comparing to Method 1.\n",
    "\n",
    "We will roughly implement the BugLocator tool. We will use the same preprocessing as TF-IDF code we developed for method 1 to calculate an indirect relevancy function. Then, we will use a weighted average of the direct relevancy function and indirect relevancy function to do the ranking for this method. The indirect function calculates the similarity between the new bug report and the historical ones. Then, given that we already know which exact files have been fixed for each historical bug report. So, we can map files to historical bug reports. Then, the algorithm ranks source files according to their indirect similarity (the similarity of a source file's corresponding historical report(s) to the new bug report) to the new bug report.\n",
    "\n",
    "- Method 2 MUST improve method 1 results.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 3\n",
    "\n",
    "This is our brand new FL technique applicable on this dataset. The novel approach should use a machine learning/information retrieval method that is not taught in class. It is okay if the method is already proposed in the FL literature and is published, however, your code cannot be copy-pasted. This method does not need to outperform the other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59492509ee09bb1e98cb3b73aca52e57ed875f73ef434eaac26b9b50866a2d0e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
