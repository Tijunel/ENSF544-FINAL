{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Step 0 - Preprocessing</h1></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we first read the data including the bug reports and source code files of all 12 projects and for ease of access, we save them as two pickle files in the ./Output directory. Therefore, this set of code will populate the ./Output directory with \"allBugReports.pickle\" which is a pandas dataframe that contains all the bug reports from all projects and \"allSourceCodes.pickle\" which is a pandas dataframe that contains all source files after preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: javalang in /Users/justintijunelis/opt/anaconda3/lib/python3.8/site-packages (0.13.0)\n",
      "Requirement already satisfied: six in /Users/justintijunelis/opt/anaconda3/lib/python3.8/site-packages (from javalang) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install javalang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import warnings\n",
    "import javalang\n",
    "import re\n",
    "import glob\n",
    "import math\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "from tqdm.notebook import tqdm as tq\n",
    "from time import gmtime, strftime\n",
    "from random import randint\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Splitting code and natural language</h1></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Loading source codes into pandas Dataframe</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classNames_methodNames(node):\n",
    "    result=''\n",
    "    if isinstance(node,javalang.tree.MethodDeclaration) or isinstance(node,javalang.tree.ClassDeclaration):\n",
    "        return node.name.lower()+' '\n",
    "    if not (isinstance(node,javalang.tree.PackageDeclaration) or\n",
    "        isinstance(node,javalang.tree.FormalParameter) or\n",
    "       isinstance(node,javalang.tree.Import)):\n",
    "        if node:\n",
    "            if isinstance(node, javalang.ast.Node):\n",
    "                for childNode in node.children:\n",
    "                    result+=classNames_methodNames(childNode)\n",
    "    return result\n",
    "    \n",
    "def traverse_node(node,i=0):\n",
    "    i+=1\n",
    "    result=''\n",
    "    if not(isinstance(node,javalang.tree.PackageDeclaration)\n",
    "            or isinstance(node,javalang.tree.FormalParameter)            \n",
    "            or isinstance(node,javalang.tree.Import)\n",
    "            or isinstance(node,javalang.tree.CompilationUnit)):\n",
    "        if node:\n",
    "            if (isinstance(node,int) or isinstance(node,str) or isinstance(node,float)) and i==2:\n",
    "                result+=node+' '\n",
    "            if isinstance(node, javalang.ast.Node):\n",
    "                for childNode in node.children:\n",
    "                    result+=traverse_node(childNode,i)\n",
    "    return result\n",
    "\n",
    "def code_parser(code):\n",
    "    try:\n",
    "        tree = javalang.parse.parse(code)\n",
    "        return ''.join([traverse_node(node) for path, node in tree]) + ' ' + ''.join([classNames_methodNames(node)\n",
    "                                                                                      for path, node in tree])\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        return ''\n",
    "\n",
    "def loadSourceFiles2df(PATH,project):\n",
    "    \"\"\"\n",
    "    Receives: group name and project name \n",
    "    Process: open the source file directory and finds all the java files,\n",
    "             and after preprocessing(using code_preprocessor) load them into a pandas dataframe \n",
    "    Returns: dataframe >> \"filename\",\"code\",\"size\"\n",
    "    \"\"\"\n",
    "    print('Loading source files of {}  ...'.format(project))\n",
    "    PATH=os.path.join(\"data\",project,\"gitrepo\")\n",
    "    all_source_files=glob.glob(PATH+'/**/*.java', recursive=True)\n",
    "    source_codes_df=pd.DataFrame([])\n",
    "    sourceCodesList=[]\n",
    "\n",
    "    for filename in tq(all_source_files):\n",
    "        code=open(filename,encoding='ISO-8859-1').read()\n",
    "        if 'src/' in filename:\n",
    "            sourceCodesList.append(dict({\"filename\":filename.split('src/')[1].replace('/','.').lower(),\n",
    "                                         \"unprocessed_code\":code,\n",
    "                                         'project':project}))\n",
    "        else:\n",
    "            sourceCodesList.append(dict({\"filename\":filename.split(project)[1].replace('/','.').lower(),\n",
    "                                         \"unprocessed_code\":code,\n",
    "                                         'project':project}))\n",
    "    source_codes_df=source_codes_df.append(pd.DataFrame(sourceCodesList))\n",
    "    return source_codes_df\n",
    "\n",
    "def load_all_SCs(dataPath):\n",
    "    print('\\tLoading all source codes ... ')\n",
    "    source_codes_df=pd.DataFrame([])\n",
    "    all_projects= [folder for folder in listdir(dataPath)]\n",
    "    for project in all_projects:\n",
    "        source_path=os.path.join(dataPath,project,\"gitrepo\")\n",
    "        source_codes_df=source_codes_df.append(loadSourceFiles2df(source_path,project))\n",
    "    return source_codes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Loading bug reports pandas Dataframe</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadBugs2df(PATH,project):\n",
    "    \"\"\"\n",
    "    @Receives: the path to bug repository (the xml file)\n",
    "    @Process: Parses the xml file and reads the fix files per bug id. \n",
    "    @Returns: Returns the dataframe\n",
    "    \"\"\"\n",
    "    print(\"Loading Bug reports ... \")\n",
    "    all_bugs_df=pd.DataFrame([],columns=[\"id\",\"fix\",\"text\",\"fixdate\"])\n",
    "    bugRepo = ET.parse(PATH).getroot()\n",
    "    buglist=[]                   \n",
    "    for bug in tq(bugRepo.findall('bug')):\n",
    "        bugDict=dict({\"id\":bug.attrib['id'],\"fix\":[],\"fixdate\":bug.attrib['fixdate']\n",
    "                      ,\"summary\":None,\"description\":None,\"project\":project,\"average_precision\":0.0})\n",
    "        for bugDetail in bug.find('buginformation'):\n",
    "            if bugDetail.tag=='summary':\n",
    "                bugDict[\"summary\"]=bugDetail.text\n",
    "            elif bugDetail.tag=='description':\n",
    "                bugDict[\"description\"]=bugDetail.text\n",
    "        bugDict[\"fix\"]=np.array([fixFile.text.replace('/','.').lower() for fixFile in bug.find('fixedFiles')])\n",
    "        summary=str(bugDict['summary']) if str(bugDict['summary']) !=np.nan else \"\"\n",
    "        description=str(bugDict['description']) if str(bugDict['description']) !=np.nan else \"\"\n",
    "        buglist.append(bugDict)\n",
    "    all_bugs_df=all_bugs_df.append(pd.DataFrame(buglist))\n",
    "    return all_bugs_df.set_index('id')\n",
    "\n",
    "def load_all_BRs(dataPath):\n",
    "    print('\\tLoading all bug reports ... ')\n",
    "    all_bugs_df=pd.DataFrame([])\n",
    "    all_projects= [folder for folder in listdir(dataPath)]\n",
    "    for project in all_projects:\n",
    "        data_path=os.path.join(dataPath,project,\"bugrepo\",\"repository.xml\")\n",
    "        all_bugs_df=all_bugs_df.append(loadBugs2df(data_path,project))\n",
    "        print(len(all_bugs_df))\n",
    "    return all_bugs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><h1>Main Preprocessing class</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingUnit:\n",
    "    all_projects_source_codes=pd.DataFrame([])\n",
    "    all_projects_bugreports=pd.DataFrame([])\n",
    "    \n",
    "    def __init__(self,dataPath):\n",
    "        self.dataPath=dataPath\n",
    "        self.dataFolder=os.path.join(os.getcwd(),'Output')\n",
    "        if not os.path.exists(self.dataFolder):\n",
    "            os.makedirs(self.dataFolder)\n",
    "            \n",
    "    def execute(self):\n",
    "        self.loadEverything()\n",
    "\n",
    "    def loadEverything(self):\n",
    "        vectorize=False\n",
    "        if PreprocessingUnit.all_projects_bugreports.empty:\n",
    "            bugReportFile=os.path.join(self.dataFolder,'allBugReports.pickle')\n",
    "            if not os.path.isfile(bugReportFile):\n",
    "                PreprocessingUnit.all_projects_bugreports=load_all_BRs(dataPath=self.dataPath)\n",
    "                vectorize=True\n",
    "                PreprocessingUnit.all_projects_bugreports.to_pickle(bugReportFile)\n",
    "            else: \n",
    "                PreprocessingUnit.all_projects_bugreports=pd.read_pickle(bugReportFile)\n",
    "        print(\"*** All bug reports are are preprocessed and stored as: {} ***\".format('/'.join(bugReportFile.split('/')[-2:])))\n",
    "\n",
    "        if PreprocessingUnit.all_projects_source_codes.empty:\n",
    "            sourceCodeFile=os.path.join(self.dataFolder,'allSourceCodes.pickle')\n",
    "            if not os.path.isfile(sourceCodeFile):\n",
    "                PreprocessingUnit.all_projects_source_codes=load_all_SCs(dataPath=self.dataPath)\n",
    "                vectorize=True\n",
    "                PreprocessingUnit.all_projects_source_codes.to_pickle(sourceCodeFile)\n",
    "            else:\n",
    "                PreprocessingUnit.all_projects_source_codes=pd.read_pickle(sourceCodeFile)\n",
    "        print(\"*** All source codes are preprocessed and stored as: {} ***\".format('/'.join(sourceCodeFile.split('/')[-2:])))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** All bug reports are are preprocessed and stored as: Output/allBugReports.pickle ***\n",
      "*** All source codes are preprocessed and stored as: Output/allSourceCodes.pickle ***\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    config={'DATA_PATH':os.path.join('data')}\n",
    "    preprocessor=PreprocessingUnit(dataPath=config['DATA_PATH'])\n",
    "    preprocessor.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** All Bug Reports are Loaded. ***\n",
      "*** All Source Codes are Loaded. ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1858"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10461"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def loadEverything():\n",
    "    all_projects_bugreports = pd.read_pickle('Output/allBugReports.pickle')\n",
    "    print(\"*** All Bug Reports are Loaded. ***\")\n",
    "    all_projects_source_codes = pd.read_pickle('Output/allSourceCodes.pickle')\n",
    "    print(\"*** All Source Codes are Loaded. ***\")\n",
    "    return all_projects_bugreports, all_projects_source_codes\n",
    "\n",
    "all_projects_bugreports, all_projects_source_codes = loadEverything()\n",
    "display(len(all_projects_bugreports))\n",
    "display(len(all_projects_source_codes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem\n",
    "\n",
    "There are several software engineering (SE) problems that can be investigated using machine learning. Among them, we will be working on a problem called \"Fault Localization\" (FL). The goal of FL is to automatically locate a fault entity (e.g. a source file, a class, a method, etc) in source code. There are different variations of FL and we will focus on Information Retrieval based FL (IRFL). This article explains FL: https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=2530&context=sis_research\n",
    "\n",
    "In short, the idea is, given a new bug report document, we want to automatically identify the source code file that most likely needs a fix, so we can save time for debugging. \n",
    "\n",
    "To do this, we may use the previous bug resports and identify the locations (files) that have been patched as our training set. So, we build an IRFL model that:\n",
    "\n",
    "- Finds the textual similarity between the new bug report and the historical ones. \n",
    "- Then rank historically patched source files based on how similar their bug reports are to the new bug report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's import some things that will help us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/justintijunelis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/justintijunelis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Key Imports\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from collections import defaultdict\n",
    "\n",
    "import multiprocessing as mp\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download some stuff to run the code\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "cores = mp.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "\n",
    "First, we need to create our training set. Using `all_projects_bug_reports` and `all_projects_source_codes`.\n",
    "\n",
    "We will clean the bug report and source code text by creating a function that:\n",
    "\n",
    "1. Makes all text lowercase\n",
    "2. Removes all punctuation from the text\n",
    "3. Removes all repetitive white space from the text\n",
    "4. Tokenizes the filtered string and removes stem words\n",
    "\n",
    "Then, we will extract the features and labels of the bug report by:\n",
    "\n",
    "1. Concatenating the bug summary and description, then using the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  # Remove imports\n",
    "  text = re.sub(r\"import\\s.*;\", \"\", text)\n",
    "  # Remove packages\n",
    "  text = re.sub(r\"package\\s.*;\", \"\", text)\n",
    "  # Space out camel case (https://stackoverflow.com/questions/5020906/python-convert-camel-case-to-space-delimited-using-regex-and-taking-acronyms-in)\n",
    "  text = re.sub(r\"((?<=[a-z])[A-Z]|(?<!\\A)[A-Z](?=[a-z]))\", r\" \\1\", text)\n",
    "  # Replace punctuation with spaces (https://stackoverflow.com/questions/68590438/replace-punctuation-with-space-in-text)\n",
    "  text = re.sub(r\"(?:[^\\w\\s]|_)+\", \" \", text)\n",
    "  # Replace white space or repeating whitespace with single space\n",
    "  text = re.sub(\"\\s+\", \" \", text) \n",
    "  # Remove all numbers\n",
    "  text = re.sub(r\"[0-9]+\", \"\", text)\n",
    "  # Remove HTML tags? # Remove static, int, char, etc\n",
    "  \n",
    "  # Make everything lowercase\n",
    "  text = text.lower()\n",
    "\n",
    "  # Tokenize the words and remove all stop words, ensure no punctuation goes though\n",
    "  banned_tokens = [\"copyright\", \"void\"]\n",
    "  tokenized = word_tokenize(text) \n",
    "  tokens = []\n",
    "  for token in tokenized:\n",
    "    if token not in stopwords.words('english') and token not in banned_tokens:\n",
    "      tokens.append(token)\n",
    "\n",
    "  # Stem all words using Porter Stemming\n",
    "  stemmer = PorterStemmer()\n",
    "  tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "  # Recreate the text\n",
    "  return \" \".join(tokens)\n",
    "\n",
    "def process_bug_reports(bug_reports):\n",
    "  for row, bug_report in bug_reports.iterrows():\n",
    "    description = bug_report['description']\n",
    "    summary = bug_report['summary']\n",
    "    bug_text = \"\"\n",
    "    if isinstance(description, str):\n",
    "      bug_text += description\n",
    "    if isinstance(summary, str):\n",
    "      bug_text += summary\n",
    "    bug_text = clean_text(bug_text)\n",
    "\n",
    "    # Skip and delete the row if the bug text is empty\n",
    "    if bug_text == \"\":\n",
    "      bug_reports.drop(row, inplace = True)\n",
    "    else:\n",
    "      bug_reports.at[row, 'text'] = bug_text\n",
    "  bug_reports.drop('summary', axis = 1, inplace = True)\n",
    "  bug_reports.drop('description', axis = 1, inplace = True)\n",
    "  return bug_reports\n",
    "\n",
    "def process_source_files(source_files):\n",
    "  source_files['unprocessed_code'] = source_files['unprocessed_code'].apply(lambda t: clean_text(t))\n",
    "  source_files.rename(columns={\"unprocessed_code\": \"code\"}, inplace = True)\n",
    "  source_files = source_files[source_files['code'] != \"\"]\n",
    "  return source_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the bug reports and source files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  with mp.get_context('fork').Pool(processes = cores) as p:\n",
    "    bug_reports = pd.concat(\n",
    "      p.map(\n",
    "        process_bug_reports, \n",
    "        np.array_split(all_projects_bugreports.copy(), cores)\n",
    "      )\n",
    "    )\n",
    "  with mp.get_context('fork').Pool(processes = cores) as p:\n",
    "    source_files = pd.concat(\n",
    "      p.map(\n",
    "        process_source_files, \n",
    "        np.array_split(all_projects_source_codes.copy(), cores)\n",
    "      )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1\n",
    "- You preprocess the data to have a clean dataset representing source files (including the buggy ones) and the bug reports. The exact preprocessing choices are ours to make.\n",
    "- Next, apply the TF-IDF method to calculate the similarity between the new bug report (to locate) and the source code files directly. Unlike BugLocator, we ignore the historical bug reports in this step. The similarity function of Method 1 is called the direct relevancy function.\n",
    "- Finally, we rank the source files based on their textual similarity to the new bug report and present the results using proper evaluation metrics (such as MAP and MRR).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Similarities\n",
    "\n",
    "Since our dataset has multiple projects, and each project has multiple bug reports, we don't want to compare bug reports with files of a project they don't belong to. So, we will compare a bug report to all the files in its respective project. \n",
    "\n",
    "To compute similarity between a bug report and it's project file:\n",
    "\n",
    "1. Iterate through each file of the bug report's project.\n",
    "2. Create a TF-IDF vectorizer and fit and transform the file's source code since we want to compare against the source code.\n",
    "3. Transform the bug report's text with the vectorizer.\n",
    "4. Compare the similarity of the two resulting vectors using cosine distance.\n",
    "\n",
    "We will iterate through each bug report and generate its similarity, then return a list of similarities that will implicitely map to each bug report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(bug_report, source_files):\n",
    "  # Find the similarity score\n",
    "  vectorizer = TfidfVectorizer()\n",
    "  source_vector = vectorizer.fit_transform(source_files['code'])\n",
    "  bug_vector = vectorizer.transform([bug_report['text']])\n",
    "  similarity_score = cosine_similarity(source_vector, bug_vector)\n",
    "\n",
    "  # Extract the score\n",
    "  scores = []\n",
    "  for score in similarity_score:\n",
    "    scores.append(score[0])\n",
    "\n",
    "  # Sort the values\n",
    "  df = pd.DataFrame()\n",
    "  df['scores'] = np.array(scores)\n",
    "  df['files'] = source_files['filename'].values\n",
    "  df = df.sort_values('scores', ascending = False)\n",
    "\n",
    "  # Create a tuple array\n",
    "  tuples = []\n",
    "  for file, score in zip(df['files'].values, df['scores'].values):\n",
    "    tuples.append((file, score))\n",
    "  return tuples\n",
    "\n",
    "def compute_similarities(bug_reports, source_files):\n",
    "  for row, bug_report in bug_reports.iterrows():\n",
    "    similarity = calculate_similarity(bug_report, source_files)\n",
    "    bug_reports.at[row, 'similarities'] = [similarity]\n",
    "  return bug_reports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the similarities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  project_bug_reports = [group for _, group in bug_reports.groupby('project')]\n",
    "  project_source_files = [group for _, group in source_files.groupby('project')]\n",
    "  bug_reports['similarities'] = None\n",
    "  with mp.get_context('fork').Pool(processes = len(project_bug_reports)) as p:\n",
    "    bug_reports = pd.concat(\n",
    "      p.starmap(\n",
    "        compute_similarities,\n",
    "        zip(project_bug_reports, project_source_files)\n",
    "      )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's check the MRR scores\n",
    "\n",
    "Our dataset doesn't quite fit with the MRR formula, so we have to improvise. The MRR formula assumes that there is only one query element for a search set (e.g. for set A, B, C, we have a query of A). Since we have a list of queries (e.g. for a set A, B, C, we have a query A, B), our query is a subset of the search set, rather than an element. \n",
    "\n",
    "If our query is A, B and the search set is A, B, C, we expect our MRR to be 1. However, using the formula would give us 3/4. So, what we can do is iterate through each element of the query and calculate an MRR, then, *remove* that query element from the subset. \n",
    "\n",
    "It looks something like this:\n",
    "\n",
    "- Query: A, B -> Search Set: A, B, C\n",
    "- Calculate MRR for A -> 1/1\n",
    "- Remove A from the Search Set, which becomes: B, C\n",
    "- Query A -> Search Set: B, C\n",
    "- Calculate MRR for B -> 1/1\n",
    "- Remove B from the Search Set, which becomes: C\n",
    "- Done\n",
    "- Calculate Adjusted MRR with: (MRR_A + MRR_B)/2 = 1\n",
    "\n",
    "BUT, what happens if the query is B, A? We still get an adjusted score of 3/4. To avoid this, we need to *sort* the query to be in the same order as the elements appear in the search set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def calculate_adjusted_MRR(bug_report):\n",
    "  fixed_files = bug_report['fix']\n",
    "  project_file_similarities = deepcopy(bug_report['similarities'])[0]\n",
    "\n",
    "  # Sort the fixed files to be in the same order as they appear in the project file similarities\n",
    "  i = 0\n",
    "  temp_fixed_files = []\n",
    "  for file, _ in project_file_similarities:\n",
    "    if len(temp_fixed_files) == len(fixed_files):\n",
    "      break\n",
    "    for f in fixed_files:\n",
    "      if f in file or f == file:\n",
    "        temp_fixed_files.append(file)\n",
    "  if len(temp_fixed_files) != len(fixed_files):\n",
    "    return 0\n",
    "  fixed_files = temp_fixed_files\n",
    "\n",
    "  # Calculate the MRR for the bug report\n",
    "  adjusted_reciprocals = []\n",
    "  for fixed_file in fixed_files:\n",
    "    # Find the ranking of the fixed file in the similarities array\n",
    "    similarity_index = 1\n",
    "    for i, similarity in enumerate(project_file_similarities): # check i\n",
    "      file, _ = similarity\n",
    "      if file in fixed_file:\n",
    "        similarity_index = i + 1\n",
    "        break\n",
    "\n",
    "    # Calculate the similarity and shift the window\n",
    "    adjusted_reciprocals.append(1 / similarity_index)\n",
    "    del project_file_similarities[similarity_index - 1]\n",
    "\n",
    "  return sum(adjusted_reciprocals)/len(adjusted_reciprocals)\n",
    "\n",
    "def calculate_adjusted_project_MRRs(bug_reports):\n",
    "  # Calculate the adjusted MRR for each bug report\n",
    "  for row, bug_report in bug_reports.iterrows():\n",
    "    bug_reports.at[row, 'adjusted_MRR'] = calculate_adjusted_MRR(bug_report)\n",
    "\n",
    "  # Group the project bugs, and compute the net MRR\n",
    "  project_bug_reports = [group for _, group in bug_reports.groupby('project')]\n",
    "  project_scores = defaultdict(str)\n",
    "  projects = bug_reports['project'].unique()\n",
    "  for project in projects:\n",
    "    project_scores[project] = 0\n",
    "  for project_bug_report in project_bug_reports:\n",
    "    print(\"The adjusted MRR score for project \" + project_bug_report['project'][0] + \" is: \", end=\"\")\n",
    "    score = sum(project_bug_report['adjusted_MRR'])/len(project_bug_report['adjusted_MRR'])\n",
    "    project_scores[project_bug_report['project'][0]] = score\n",
    "    print(score)\n",
    "\n",
    "  # Create a plot\n",
    "  project_scores = dict(sorted(project_scores.items(), key=lambda item: item[1]))\n",
    "  plt.figure(figsize=(20, 10))\n",
    "  plt.bar(range(len(project_scores)), list(project_scores.values()), tick_label=list(project_scores.keys()))\n",
    "  plt.xlabel('Projects')\n",
    "  plt.ylabel('Adjusted MRR Score')\n",
    "  plt.title('Adjusted MRR Score for each Project')\n",
    "  plt.show()\n",
    "\n",
    "# calculate_adjusted_project_MRRs(bug_reports)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's calculate the MAP scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_precision(bug_report):\n",
    "  fixed_files = bug_report['fix']\n",
    "  documents = bug_report['similarities'][0]\n",
    "  average_precision = []\n",
    "  for j in range(len(documents)):\n",
    "    if j + 1 == len(documents):\n",
    "      break\n",
    "\n",
    "    # Calculate P(j)\n",
    "    top_j_documents = documents[0:j]\n",
    "    number_of_positive_instances_in_top_j = 0\n",
    "    for i in range(len(fixed_files)):\n",
    "      for k in range(i, j):\n",
    "        if fixed_files[i] in top_j_documents[k][0] or fixed_files[i] == top_j_documents[k][0]:\n",
    "          number_of_positive_instances_in_top_j += 1\n",
    "      \n",
    "    p_j = number_of_positive_instances_in_top_j / (j + 1)\n",
    "\n",
    "    # Calculate pos(j)\n",
    "    pos_j = 0\n",
    "    document_j = documents[j]\n",
    "    for file in fixed_files:\n",
    "      if file in document_j[0] or file == document_j[0]:\n",
    "        pos_j = 1\n",
    "\n",
    "    average_precision.append((p_j * pos_j) / len(fixed_files))\n",
    "\n",
    "  return sum(average_precision)\n",
    "\n",
    "def calculate_adjusted_project_MAP_scores(bug_reports):\n",
    "  # Calculate the average precision for each bug report\n",
    "  for row, bug_report in bug_reports.iterrows():\n",
    "    score = calculate_average_precision(bug_report)\n",
    "    print(score)\n",
    "    bug_reports.at[row, 'average_precision'] = score\n",
    "\n",
    "  # Group the project bugs, and compute the net MAP per project\n",
    "  project_bug_reports = [group for _, group in bug_reports.groupby('project')]\n",
    "  project_scores = defaultdict(str)\n",
    "  projects = bug_reports['project'].unique()\n",
    "  for project in projects:\n",
    "    project_scores[project] = 0\n",
    "  for project_bug_report in project_bug_reports:\n",
    "    print(\"The MAP score for project \" + project_bug_report['project'][0] + \" is: \", end=\"\")\n",
    "    score = sum(project_bug_report['average_precision'])/len(project_bug_report['average_precision'])\n",
    "    project_scores[project_bug_report['project'][0]] = score\n",
    "    print(score)\n",
    "\n",
    "  # Create the plot\n",
    "  project_scores = dict(sorted(project_scores.items(), key=lambda item: item[1]))\n",
    "  plt.figure(figsize=(20, 10))\n",
    "  plt.bar(range(len(project_scores)), list(project_scores.values()), tick_label=list(project_scores.keys()))\n",
    "  plt.xlabel('Projects')\n",
    "  plt.ylabel('MAP Score')\n",
    "  plt.title('MAP Score for each Project')\n",
    "  plt.show()\n",
    "\n",
    "calculate_adjusted_project_MAP_scores(bug_reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2\n",
    "\n",
    "In this step, we will develop a new IRFL method and comparing to Method 1.\n",
    "\n",
    "We will roughly implement the BugLocator tool. We will use the same preprocessing as TF-IDF code we developed for method 1 to calculate an indirect relevancy function. Then, we will use a weighted average of the direct relevancy function and indirect relevancy function to do the ranking for this method. The indirect function calculates the similarity between the new bug report and the historical ones. Then, given that we already know which exact files have been fixed for each historical bug report. So, we can map files to historical bug reports. Then, the algorithm ranks source files according to their indirect similarity (the similarity of a source file's corresponding historical report(s) to the new bug report) to the new bug report.\n",
    "\n",
    "- Method 2 MUST improve method 1 results.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 3\n",
    "\n",
    "This is our brand new FL technique applicable on this dataset. The novel approach should use a machine learning/information retrieval method that is not taught in class. It is okay if the method is already proposed in the FL literature and is published, however, your code cannot be copy-pasted. This method does not need to outperform the other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59492509ee09bb1e98cb3b73aca52e57ed875f73ef434eaac26b9b50866a2d0e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
